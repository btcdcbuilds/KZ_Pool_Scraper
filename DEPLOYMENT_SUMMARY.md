# KZ Pool Scraper - Deployment Summary

## âœ… Project Complete & Tested

**Date**: November 23, 2025  
**Status**: Production Ready  
**GitHub**: https://github.com/btcdcbuilds/KZ_Pool_Scraper  
**Release**: v1.1.0

---

## What Was Built

### 1. Web Scraper (`btcpool_scraper.py`)
- Scrapes KZ Pool observer page using Playwright
- Extracts pool summary, worker status, daily earnings
- Saves to local SQLite database
- Detects anomalies (offline workers, hashrate drops)
- **Tested**: Successfully scraped 200 workers

### 2. Supabase Uploader (`upload_to_supabase.py`)
- Reads from SQLite database
- Uploads to Supabase cloud database
- Compatible with existing Antpool scraper schema
- Batch inserts for performance
- **Tested**: Successfully uploaded 200 workers to Supabase

### 3. Automation Script (`run_scraper_and_upload.sh`)
- Combined scrape + upload in one command
- Configurable via environment variables
- Logging to file
- Error handling
- **Ready**: For cron job every 10 minutes

### 4. Documentation
- `README.md` - Main project overview
- `README_SUPABASE.md` - Supabase quick start
- `SUPABASE_INTEGRATION.md` - Detailed integration guide
- `DATABASE_SCHEMA.md` - Schema reference
- `INTEGRATION_GUIDE.md` - Cross-repository integration
- `INSTALLATION.md` - Installation instructions

---

## Test Results

### Scraper Test
```
âœ“ Summary fields: 6
âœ“ Workers found: 200
âœ“ Daily earnings records: 4
âœ“ Anomaly detected: 2 offline workers
âœ“ Saved to SQLite: btcpool_data.db
```

### Supabase Upload Test
```
âœ“ Account created: KZPool_SUCCESS
âœ“ Balance: 0.00401902 BTC
âœ“ 24h Earnings: 0.01570138 BTC
âœ“ Hashrate: 38.384 PH/s (current), 38.185 PH/s (24h avg)
âœ“ Workers: 200 devices uploaded
âœ“ Income tracking: 3 daily records
âœ“ Alerts: Offline worker anomaly logged
```

### Database Verification
```sql
-- Verified in Supabase
SELECT * FROM accounts WHERE account_name = 'KZPool_SUCCESS';
-- Result: 1 row with correct data

SELECT COUNT(*) FROM devices WHERE account_name = 'KZPool_SUCCESS';
-- Result: 200 workers

SELECT * FROM hashrates WHERE account_name = 'KZPool_SUCCESS';
-- Result: Hashrate data with 38384 TH/s
```

---

## Supabase Schema Integration

Uses existing tables from Antpool scraper:

| Table | Purpose | Records |
|-------|---------|---------|
| `accounts` | Pool account info | 1 per pool |
| `hashrates` | Hashrate metrics | 1 per scrape (every 10 min) |
| `devices` | Worker/device status | 200 per pool |
| `income_tracking` | Daily earnings | 1 per day |
| `alerts` | Anomaly logging | As needed |

**Compatible**: Both Antpool and KZ Pool scrapers use the same schema.

---

## Deployment Options

### Option 1: VPS (Recommended)

```bash
# Clone repository
git clone https://github.com/btcdcbuilds/KZ_Pool_Scraper.git
cd KZ_Pool_Scraper

# Install
./install.sh

# Configure
export SUPABASE_URL="https://your-project.supabase.co"
export SUPABASE_SERVICE_KEY="your-service-role-key"
export ACCOUNT_NAME="KZPool_001"
export CLIENT_NAME="Your Client"

# Test
./run_scraper_and_upload.sh

# Automate (every 10 minutes)
crontab -e
# Add: */10 * * * * cd /path/to/KZ_Pool_Scraper && ./run_scraper_and_upload.sh >> /var/log/kzpool.log 2>&1
```

### Option 2: Docker

```bash
# Clone repository
git clone https://github.com/btcdcbuilds/KZ_Pool_Scraper.git
cd KZ_Pool_Scraper

# Configure .env file
cat > .env << EOF
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_SERVICE_KEY=your-service-role-key
ACCOUNT_NAME=KZPool_001
CLIENT_NAME=Your Client
EOF

# Run
docker-compose up -d

# View logs
docker-compose logs -f
```

---

## Configuration

### Environment Variables

```bash
# Required
export SUPABASE_URL="https://idlmoqiftasxyuekeyjm.supabase.co"
export SUPABASE_SERVICE_KEY="eyJhbGci..."

# Optional (defaults shown)
export ACCOUNT_NAME="KZPool_001"
export CLIENT_NAME="Client Name"
export COMPANY="BTCDC Builds"
export COUNTRY="Kazakhstan"
export SITE="KZ Pool"
export DB_PATH="btcpool_data.db"
```

### Multiple Pools

To monitor multiple pools, run separate instances:

```bash
# Pool 1
ACCOUNT_NAME="KZPool_001" CLIENT_NAME="Client A" ./run_scraper_and_upload.sh

# Pool 2
ACCOUNT_NAME="KZPool_002" CLIENT_NAME="Client B" ./run_scraper_and_upload.sh
```

Or use `pools_config.json` with the management script.

---

## Monitoring

### View Logs

```bash
tail -f /var/log/kzpool.log
```

### Check SQLite Database

```bash
sqlite3 btcpool_data.db "SELECT * FROM pool_summary ORDER BY timestamp DESC LIMIT 1"
```

### Query Supabase

```bash
curl -X GET "https://your-project.supabase.co/rest/v1/accounts?account_name=eq.KZPool_001" \
  -H "apikey: YOUR_KEY" \
  -H "Authorization: Bearer YOUR_KEY" | jq
```

### View Data

```bash
python view_data.py
python view_data.py offline
python view_data.py anomalies
```

---

## Performance

- **Scrape time**: ~15 seconds
- **Upload time**: ~5 seconds (200 workers)
- **Total cycle**: ~20 seconds
- **Database growth**: ~1.4 MB per day
- **Memory usage**: ~200 MB during scraping
- **CPU usage**: <5% average

**Recommendation**: Run every 10 minutes (144 times per day)

---

## Integration with Existing System

### Antpool Scraper Compatibility

Both scrapers use the same Supabase schema:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Antpool    â”‚â”€â”€â”€â”€â–¶â”‚  Supabase   â”‚â—€â”€â”€â”€â”€â”‚  KZ Pool    â”‚
â”‚  Scraper    â”‚     â”‚  Database   â”‚     â”‚  Scraper    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Dashboard  â”‚
                    â”‚  Analytics  â”‚
                    â”‚ Telegram Botâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Query Examples

```sql
-- Total hashrate across all pools
SELECT 
  account_type,
  SUM(hashrate_1d) / 1000 as total_ph,
  SUM(active_workers) as workers
FROM hashrates h
JOIN accounts a ON h.account_name = a.account_name
WHERE h.timestamp > NOW() - INTERVAL '1 hour'
GROUP BY account_type;

-- All offline workers
SELECT 
  account_name,
  worker_name,
  location,
  updated_at
FROM devices
WHERE status = 'offline'
ORDER BY updated_at DESC;

-- Daily earnings by pool
SELECT 
  account_name,
  SUM(btc_amount) as total_btc,
  COUNT(*) as days
FROM income_tracking
WHERE date > NOW() - INTERVAL '30 days'
GROUP BY account_name;
```

---

## Next Steps

### Phase 1: Monitoring (Current)
- âœ… Scraper working
- âœ… Supabase integration
- âœ… Automation ready

### Phase 2: Alerts (Next)
- [ ] Telegram bot integration
- [ ] Email alerts
- [ ] SMS notifications
- [ ] Webhook support

### Phase 3: Dashboard (Future)
- [ ] Web UI for monitoring
- [ ] Real-time charts
- [ ] Historical analytics
- [ ] Multi-pool comparison

### Phase 4: Advanced (Future)
- [ ] Predictive analytics
- [ ] Earnings projections
- [ ] Performance optimization
- [ ] API for third-party integration

---

## Troubleshooting

### Common Issues

**Problem**: Scraper fails to load page  
**Solution**: Check internet connection, increase timeout

**Problem**: Upload fails with "Invalid API key"  
**Solution**: Verify `SUPABASE_SERVICE_KEY` is service role key

**Problem**: No workers extracted  
**Solution**: Page structure may have changed, update selectors

**Problem**: Duplicate device errors  
**Solution**: Devices are keyed by `device_id`, duplicates are normal

### Debug Mode

```bash
# Run scraper with debug output
python btcpool_scraper.py 2>&1 | tee scraper_debug.log

# Run upload with debug output
python upload_to_supabase.py --account "Test" --client "Test" 2>&1 | tee upload_debug.log
```

---

## Support

- **GitHub Issues**: https://github.com/btcdcbuilds/KZ_Pool_Scraper/issues
- **Documentation**: See `SUPABASE_INTEGRATION.md`
- **Logs**: Check `/var/log/kzpool.log`

---

## Credits

**Built by**: BTCDC Builds  
**Purpose**: Bitcoin mining pool monitoring and management  
**License**: MIT

---

## Summary

âœ… **Scraper**: Working and tested with 200 workers  
âœ… **Supabase**: Successfully integrated with existing schema  
âœ… **Automation**: Ready for 10-minute cron job  
âœ… **Documentation**: Complete guides and examples  
âœ… **GitHub**: Pushed to repository with v1.1.0 release  

**Status**: Production ready for deployment on your VPS! ðŸš€

